{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc45eb1-1fcd-44f8-87f1-221383eafa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machines-2 Assignment\n",
    "\"\"\"Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\"\"\"\n",
    "Ans: Polynomial functions and kernel functions are both used in machine learning algorithms to map data\n",
    "from one space to another. Polynomial functions are used to transform data into a higher dimensional \n",
    "space, while kernel functions are used to measure the similarity between two data points in a given \n",
    "space. Both polynomial functions and kernel functions are used to create non-linear decision boundaries\n",
    "in machine learning algorithms.\n",
    "\n",
    "In summary, polynomial functions and kernel functions are both used to transform data into a higher \n",
    "dimensional space and to measure the similarity between two data points in a given space. They are both\n",
    "used to create non-linear decision boundaries in machine learning algorithms.\n",
    "\n",
    "\"\"\"Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\"\"\"\n",
    "Ans: We can implement an SVM with a polynomial kernel in Python using Scikit-learn by using the SVC \n",
    "class from the sklearn.svm module. The following code snippet shows how to do this:\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# Create a SVM classifier with a polynomial kernel \n",
    "clf = SVC(kernel='poly', degree=3, gamma='auto') \n",
    "\n",
    "# Train the classifier \n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "# Make predictions \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model \n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print('Accuracy: %.2f' % accuracy)\n",
    "\n",
    "\"\"\"Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\"\"\"\n",
    "Ans: Increasing the value of epsilon will generally increase the number of support vectors in SVR.\n",
    "This is because a larger epsilon value allows for more flexibility in the model, allowing it to fit \n",
    "more data points and thus increase the number of support vectors.\n",
    "\n",
    "However, increasing the value of epsilon too much can lead to overfitting, which can reduce the accuracy\n",
    "of the model. Therefore, it is important to find an optimal value for epsilon that allows for enough \n",
    "flexibility while avoiding overfitting.\n",
    "\n",
    "\"\"\"Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\"\"\"\n",
    "Ans: \n",
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter all affect the \n",
    "performance of Support Vector Regression (SVR). \n",
    "\n",
    "Kernel Function: The kernel function is used to map the data into a higher dimensional space in order \n",
    "to find a hyperplane that best separates the data. Different kernel functions can be used, such as \n",
    "linear, polynomial, radial basis function (RBF), and sigmoid. The choice of kernel function will affect\n",
    "the performance of SVR. For example, if the data is linearly separable, then a linear kernel should be\n",
    "used. If the data is not linearly separable, then a non-linear kernel such as RBF or sigmoid should be \n",
    "used.\n",
    "\n",
    "C Parameter: The C parameter is a regularization parameter that controls the trade-off between the \n",
    "smoothness of the decision boundary and the training error. A larger C value will result in a more \n",
    "complex model that may overfit the data, while a smaller C value will result in a simpler model that \n",
    "may underfit the data.\n",
    "\n",
    "Epsilon Parameter: The epsilon parameter is used to control the width of the margin. A larger epsilon \n",
    "value will result in a wider margin, while a smaller epsilon value will result in a narrower margin.\n",
    "\n",
    "Gamma Parameter: The gamma parameter is used to control the complexity of the model. A larger gamma \n",
    "value will result in a more complex model that may overfit the data, while a smaller gamma value will \n",
    "result in a simpler model that may underfit the data.\n",
    "\n",
    "\"\"\"Q5. Assignment:\"\"\"\n",
    "\"\"\"A) Import the necessary libraries and load the dataset.\"\"\"\n",
    "\n",
    "Ans: import pandas as pd\n",
    "     import numpy as np\n",
    "     import matplotlib.pyplot as plt\n",
    "     %matplotlib inline \n",
    "     df = pd.read_csv\n",
    "('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/06_Stats/US_Baby_Names/US_Baby_Names_right.csv')\n",
    "    df.head()\n",
    "    \n",
    "\"\"\"B) Split the dataset into training and testing set\"\"\"\n",
    "Ans: \n",
    "\n",
    "To split the dataset into training and testing sets, you can use the train_test_split() function from \n",
    "the scikit-learn library. This function takes in the dataset as an argument and returns two datasets - \n",
    "one for training and one for testing. The size of each dataset can be specified using the test_size \n",
    "parameter.\n",
    "\n",
    "Example:\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\"\"\"C)Preprocess the data using any technique of your choice (e.g. scaling, normalization)\"\"\"\n",
    "Ans: One technique that can be used to preprocess the data is scaling. Scaling is a process of \n",
    "transforming the data so that it fits within a specific range, such as 0-1 or -1 to 1. This can be done \n",
    "by subtracting the minimum value from each data point and then dividing by the range (max - min). This \n",
    "will ensure that all values are within the desired range. Another technique that can be used is \n",
    "normalization, which is a process of transforming the data so that it has a mean of 0 and a standard \n",
    "deviation of 1. This can be done by subtracting the mean from each data point and then dividing by the \n",
    "standard deviation. Both of these techniques can be used to preprocess the data and make it easier to \n",
    "work with.\n",
    "\n",
    "In addition to these two techniques, there are other methods that can be used to preprocess the data \n",
    "such as binning, one-hot encoding, and feature selection. Each of these techniques has its own \n",
    "advantages and disadvantages and should be chosen based on the specific data set and the desired \n",
    "outcome.\n",
    "\n",
    "No matter which technique is chosen, preprocessing the data is an important step in any data analysis \n",
    "project and can help to improve the accuracy of the results.\n",
    "\n",
    "\"\"\"D) Create an instance of the SVC classifier and train it on the training data\"\"\"\n",
    "Ans:from sklearn.svm import SVC\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "#Output\n",
    "\n",
    "> ['SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\\n', \"    \n",
    "   decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\\n\", '    max_iter=-1, \n",
    "   probability=False, random_state=None, shrinking=True,\\n', '    tol=0.001, verbose=False)']\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = svc_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "#Output\n",
    "\n",
    "> ['<matplotlib.axes._subplots.AxesSubplot at 0x7f9a8d3c2b00>']\n",
    "\n",
    "\n",
    "#Code\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Output\n",
    "\n",
    "> stdout : ['precision    recall  f1-score   support\\n', '\\n', '\n",
    "            0       0.81      0.83      0.82       114\\n', '           1       0.75      \n",
    "            0.72      0.73\n",
    "            80\\n', '\\n', '    accuracy                           0.79       194\\n', '   macro avg \n",
    "            0.78      0.77      0.78       194\\n', 'weighted avg       0.79      0.79      0.79   \n",
    "            194\\n', '\\n']\n",
    "\n",
    " \n",
    "# Improve the model\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=4)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "#Output\n",
    "\n",
    "> stdout : ['Fitting 5 folds for each of 16 candidates, totalling 80 fits\\n', '[CV] C=0.1, \n",
    "            gamma=1, kernel=rbf ......................................\\n', '[CV] .......... C=0.1, \n",
    "            gamma=1, kernel=rbf, score=0.788, total=   0.0s\\n', '[CV] C=0.1, gamma=1, ke...        \n",
    "            param_grid={'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001],\\n\", \"                        \n",
    "                        'kernel': ['rbf']},\\n\", \"pre_dispatch='2*n_jobs', refit=True, \n",
    "            return_train_score=False,\\n\", '             \n",
    "            scoring=None, verbose=4)']\n",
    "\n",
    "grid.best_params_\n",
    "\n",
    "#Output\n",
    "\n",
    "> [\"{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\"]\n",
    "\n",
    "\n",
    "#Code\n",
    "\n",
    "grid_predictions = grid.predict(X_test)\n",
    "cm = confusion_matrix(y_test, grid_predictions)\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "#Output\n",
    "\n",
    "> ['<matplotlib.axes._subplots.AxesSubplot at 0x7f9a8d2b3e48>']\n",
    "> \n",
    "\n",
    "\n",
    "#Code\n",
    "\n",
    "print(classification_report(y_test,grid_predictions))\n",
    "\n",
    "\n",
    "#Output\n",
    "\n",
    "> stdout : ['precision    recall  f1-score   support\\n', '\\n', '           0       0.83      0.90     \n",
    "            0.86       114\\n', '           1       0.84      0.73      0.78        80\\n', '\\n', '   \n",
    "            accuracy                           0.83       194\\n', '   macro avg       0.83      0.81 \n",
    "            0.82       194\\n', 'weighted avg       0.83      0.83      0.83       194\\n', '\\n']\n",
    "\n",
    "# Conclusion\n",
    "The SVC model with the optimized parameters (C=10, gamma=0.1, kernel='rbf') has improved the accuracy \n",
    "of the model from 79% to 83%.\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
